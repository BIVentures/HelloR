-----------------------------------------------------------------------------------------------
#Purpose: Fraud Detection using Machine Learning 
#Dataset file https://www.kaggle.com/ntnu-testimon/paysim1#PS_20174392719_1491204439457_log.csv
#Author:BIVentures
#Date: March 20, 2021

######Start##########

#Step1: Load Libraries
library(plyr)
library(tidyverse)
library(caret)
library(GGally)
library(stringr)
library(RGTK2)
library(pROC)
library(ROCR)

#Step2: Set Random Seed for Reproducibility
set.seed(317)

#Step3: Load full data set 
 fraud_rawdata <- read.csv("C:/DirectoryPathHere/RFinance/fraud_log.csv")

#Step4: Take a glimpse at the raw data set
glimpse(fraud_rawdata)

#Data Pipeline
#The nameOrig (origin) and nameDest (destination) variables are technically categorical but are too many 
#Prepare Data for modelling.
#Note: The names have a letter-prefix for consistency
fraud_df <- fraud_rawdata %>%
mutate(name_orig_first = str_sub(nameOrig,1,1)) %>%
mutate(name_dest_first = str_sub(nameDest, 1, 1)) %>%
select(-nameOrig, -nameDest)


#Q1 How many unique prefixes in nameDest?
unique(fraud_df$name_dest_first)

# Two prefixes in 'nameDest' so this will be converted to a factor.
fraud_df$name_dest_first <- as.factor(fraud_df$name_dest_first)
table(fraud_df$name_dest_first)

#Q2 How many unique prefixes in nameOrig?
unique(fraud_df$name_orig_first)

#There is a single prefix in nameOrig so it is not useful and will be removed along with isFlaggedFraud 
#isFlaggedFraud looks like someone else’s prediction.
#Drop nameOrig & isFlaggedFraud columns- And re-arrange the columns into a more logical way.
fraud_df2 <- fraud_df %>%
    select(-name_orig_first, -isFlaggedFraud) %>%
    select(isFraud, type, step, everything())

#Take another Glimpse
glimpse(fraud_df2)

#The type & isFraud columns are categorical and will be changed to factors
fraud_df2$type <- as.factor(fraud_df2$type)
fraud_df2$isFraud <- as.factor(fraud_df2$isFraud)

#Some models do not like 1’s and 0’s in the result, so we will recode them for clarity
fraud_df2$isFraud <- recode_factor(fraud_df2$isFraud, `0` = "No", `1` = "Yes")

#Generate the summary
summary(fraud_df2)

#In the original dataset There are only 8213 records where isFraud is true.
#Take that into account when creating the training and test sets.
#My plan is to create a training and test dataset that is 50% Fraud & 50% not-Fraud.

#Get all fraud transactions from dataset
fraud_trans <- fraud_df2 %>%
    filter(isFraud == "Yes") 
summary(fraud_trans)

#When the type is CASH_IN, DEBIT, or PAYMENT, there are no fraud cases. 
#This should be taken into consideration when preparing training & test sets.
#In every case of fraud name_dest_first had a code of “C”.
#We can filter the main dataset to eliminate all the M’s
#Fraud Amount maxes out at 10,000,000 so we will also filter any transactions above that amount.
#Reduce main dataset
#Remove insignificant variables; filter for only CASH_OUT and TRANSFERS 
#There are also no transactions above 10,000,000  - so they can be filtered also.

fraud_df3 <- fraud_df2 %>%
    filter(type %in% c("CASH_OUT", "TRANSFER")) %>%
    filter(name_dest_first == "C") %>%
    filter(amount <= 10000000) %>%
    select(-name_dest_first)
summary(fraud_df3)

#This brings the full dataset down to 2.8M records. 
#That is a reduction of 56% in the dataset which should eliminate a lot of noise.

#Create sample dataset
not_fraud <- fraud_df3 %>%
    filter(isFraud == "No") %>%
    sample_n(8213)

is_fraud <- fraud_df3 %>%
    filter(isFraud == "Yes")

full_sample <- rbind(not_fraud, is_fraud) %>%
    arrange(step)

	
#Does step have any clear pattern in distribution across Fraud cases?
#Note that step indicates the hour within the month that this data was captured 
#so these plots should be considered time-series.
ggplot(full_sample, aes(x = step, col = isFraud)) + 
    geom_histogram(bins = 743)
	
#Outcome: There is a pattern for 'NotFraud' cases, but no descernible pattern of 'Fraud' cases from this plot.


#Look at only Fraud cases.

ggplot(is_fraud, aes(x = step)) + 
    geom_histogram(bins = 743)
	
Outcome1:There is a high positive correlation between OldBalanceOrig and newBalanceOrig.
Outcome2:Also a high correlation between oldBalanceDest and newBalanceDest.
Outcome3:We’ll deal with them systematically further down.

#Q3 Is there any pattern of fraud by Transaction Amount?
ggplot(full_sample, aes(type, amount, color = isFraud)) +
    geom_point(alpha = 0.01) + 
    geom_jitter()
	
#Outcome: No detectable patterns, It’s all over the place. No pattern.


#Look at final summary
summary(full_sample)


#Pre-processing the full dataset for modelling
preproc_model <- preProcess(fraud_df3[, -1], 
                            method = c("center", "scale", "nzv"))

fraud_preproc <- predict(preproc_model, newdata = fraud_df3[, -1])


#Bind the results to the pre-processed data
fraud_pp_w_result <- cbind(isFraud = fraud_df3$isFraud, fraud_preproc)


#Look at summary of pre-processed data
summary(fraud_pp_w_result)

#Notice that the mean of all the numeric fields is zero. 
#The standard deviation is 1. 
#This is the result of centering & scaling which puts all numeric variables on the same scale.


#Find highly correlated predictors
#Select only numeric columns
#Remove result column and categorical columns (don’t worry we’ll put them back after this test)
fraud_numeric <- fraud_pp_w_result %>%
    select(-isFraud, -type)

#Find any highly correlated predictors and remove
#Highly correlated predictors create instability in the model so one of the two is removed.

high_cor_cols <- findCorrelation(cor(fraud_numeric), cutoff = .75, verbose = TRUE, 
                                 names = TRUE, exact = TRUE)
								 

#Remove high correlation columns
high_cor_removed <- fraud_pp_w_result %>%
    select(-newbalanceDest)

#Check for linear relationships between predictors
fraud_numeric <- high_cor_removed %>%
    select(-isFraud, -type)
comboInfo <- findLinearCombos(fraud_numeric)
comboInfo

#Outcome: No linear relationships identified


####Modeling

#Copy final data to a more general dataframe
model_df <-high_cor_removed

#Create equal amount of Fraud and Not Fraud data for training
is_fraud <- model_df %>%
    filter(isFraud == "Yes")

not_fraud <- model_df %>%
    filter(isFraud == "No") %>%
    sample_n(8213)

#To mix up the sample set I'll arrange by `step`
model_full_sample <- rbind(is_fraud, not_fraud) %>%
    arrange(step)

#Split sample into train and test sets
in_train <- createDataPartition(y = model_full_sample$isFraud, p = .75, 
                               list = FALSE) 
train <- model_full_sample[in_train, ] 
test <- model_full_sample[-in_train, ] 

#Intermission for garbage collection
#Play Jeopardy theme here
gc()


#Set general parameters
#Create control used to fit all models
#We will use three iterations of 10-fold cross-validation for every model so that we can compare apples-to-apples.

control <- trainControl(method = "repeatedcv", 
                        number = 10, 
                        repeats = 3, 
                        classProbs = TRUE, 
                        summaryFunction = twoClassSummary)

						

####Approach

#In all of the models we will follow a consistent pattern.
#The pattern is:
#1. fit the model to the data
#2. compared model against the data it was trained on
#3. compared model against the test dataset that was unknown for model building
#4. compare model against a 100k sample of Not Fraud cases to determine the expected false-positives.
#In each of the models we will also track the time to create the model.
#Note: the relationship between the time to create the model and to apply the model to new data is very different.
#Model tuning, especially for black-box type models, like Support Vector Machines, 
#can take a long time, but the time to apply that model to new data is usually insignificant.
#Do not be afraid to use a lot of time to train a model if it gives better results.
#rpart model

#Recursive partitioning
#https://cran.r-project.org/web/packages/rpart/rpart.pdf
#https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf

start_time <- Sys.time()
rpart_model = train(isFraud ~ .,
                    data = train,
                    method = "rpart",
                    tuneLength = 10,
                    metric = "ROC",
                    trControl = control, 
                    parms=list(split='information'))
end_time <- Sys.time()
end_time - start_time

## Time difference of 15.95665 secs


#Predict on Training set

rpart_train_pred <- predict(rpart_model, train)
confusionMatrix(train$isFraud, rpart_train_pred)


#Accuracy = 0.9489; a lot of false-positives
#Sensitivity : 0.9832
#Specificity : 0.9191
#Predict on Test Set

#Predict on Big No-Fraud dataset
#Establish Big No-Fraud dataset

big_no_sample <- model_df %>%
    filter(isFraud == "No") %>%
    sample_n(100000)

#Predict on Big No-Fraud dataset
#Also, capture time to apply to 100,000 records

start_time <- Sys.time()
rpart_big_no_pred <- predict(rpart_model, big_no_sample)
end_time <- Sys.time()
end_time - start_time

confusionMatrix(big_no_sample$isFraud, rpart_big_no_pred)

#Accuracy : 0.9151
#Sensitivity : 1.0000
#Specificity : 0.0000
#False-positives: 8488
#Run-time: 0.2806261 secs
#Plot the ROC curve against test data

rpart_probs <- predict(rpart_model, test, type = "prob")
rpart_ROC <- roc(response = test$isFraud, 
                 predictor = rpart_probs$Yes, 
                 levels = levels(test$isFraud))

plot(rpart_ROC, col = "blue")

#Area under the curve
auc(rpart_ROC)

print(rpart_model)


####C5.0 model

#Decision trees and rule-based models for pattern recognition.
#https://cran.r-project.org/web/packages/C50/C50.pdf

grid <- expand.grid( .winnow = c(FALSE), 
                     .trials=c(50, 100, 150, 200), 
                     .model="tree" )

start_time <- Sys.time()
c5_model <- train(isFraud ~ .,
                  data = train, 
                  method = "C5.0",
                  trControl = control, 
                  metric = "ROC",
                  tuneGrid = grid, 
                  verbose = FALSE)
end_time <- Sys.time()
end_time - start_time

## Time difference of 15.54367 secs

print(c5_model)


#Predict on Training set
c5_pred_train <- predict(c5_model, train)
confusionMatrix(train$isFraud, c5_pred_train, positive = "Yes")

Accuracy : 0.9946; #WOW!!
Sensitivity : 0.9938
Specificity : 0.9954
Predict on Test set

c5_pred_test <- predict(c5_model, test)
confusionMatrix(test$isFraud, c5_pred_test, positive = "Yes")

#Predict on Test set

c5_pred_test <- predict(c5_model, test)
confusionMatrix(test$isFraud, c5_pred_test, positive = "Yes")

#Accuracy : 0.983; #WOW!! WOW!!
#Sensitivity : 0.9801
#Specificity : 0.9858
#Predict on Big No-Fraud dataset

start_time <- Sys.time()
c5_pred_big_no <- predict(c5_model, big_no_sample)
end_time <- Sys.time()
end_time - start_time

#Accuracy : 0.978
#Sensitivity : 0.00000
#Specificity : 1.00000
#False-positives: 2205
#Run-time: 5.715282 secs
#Plot the ROC curve against test data

c5_probs <- predict(c5_model, test, type = "prob")
c5_ROC <- roc(response = test$isFraud, 
                 predictor = c5_probs$Yes, 
                 levels = levels(test$isFraud))

plot(c5_ROC, col = "red")

#Area under the curve
auc(c5_ROC)


#Random Forest model
#https://cran.r-project.org/web/packages/randomForest/randomForest.pdf
#https://www.r-bloggers.com/random-forests-in-r/

grid <- expand.grid(.mtry = 5, .ntree = seq(25, 150, by = 25))

start_time <- Sys.time()
rf_model <- train(isFraud ~ ., 
                  data = train, 
                  method="rf", 
                  metric = "Accuracy", 
                  TuneGrid = grid, 
                  trControl=control)
end_time <- Sys.time()
end_time - start_time

#Visualize data
library(randomForest)
print(rf_model$finalModel)
plot(rf_model$finalModel)

#This shows that the Random Forest model is an effective learner on this dataset. 
#If this data contained a lot more variables it will take more trees to get to the plateau, 
#but it usually happens before 500 trees. The model can safely reduce the trees to 100 without 
#any significant negative performance impact.
#This plot should always be used for Random Forests to determine the best cutoff point for trees.
#Plot variable importance
varImpPlot(rf_model$finalModel)


#Predictive Analytics
#Predict on Training set
rf_train_pred <- predict(rf_model, train)
confusionMatrix(train$isFraud, rf_train_pred, positive = "Yes")

#Accuracy : 1; Obviously overfit
#Sensitivity : 1.0
#Specificity : 1.0
#Predict on Test set

rf_test_pred <- predict(rf_model, test)
confusionMatrix(test$isFraud, rf_test_pred, positive = "Yes")

#Accuracy : 0.9803; #wow!!
#Sensitivity : 0.9709
#Specificity : 0.9901
#Predict on Big No-Fraud dataset

start_time <- Sys.time()
rf_big_no_pred <- predict(rf_model, big_no_sample)
end_time <- Sys.time()
end_time - start_time

#Accuracy : 0.968
#Sensitivity : 0.00000
#Specificity : 1.00000
#False-positives: 3197
#Run-time: 2.538139 secs

confusionMatrix(big_no_sample$isFraud, rf_big_no_pred, positive = "Yes")

#Plot the ROC curve
rf_probs <- predict(rf_model, test, type = "prob")
rf_ROC <- roc(response = test$isFraud, 
                 predictor = rf_probs$Yes, 
                 levels = levels(test$isFraud))

plot(rf_ROC, col = "green")

#Area under the curve
auc(rf_ROC)


#SVM model
#Support Vector Machines
#https://cran.r-project.org/web/packages/e1071/e1071.pdf
#https://www.r-bloggers.com/machine-learning-using-support-vector-machines/

start_time <- Sys.time()
svm_model <- train(isFraud ~ ., 
                  data = train, 
                  method = "svmRadial",   # Radial kernel
                  tuneLength = 3,  # 3 values of the cost function
                  metric="ROC",
                  trControl=control)
end_time <- Sys.time()
end_time - start_time

## Time difference of 30.57211 mins

##Time difference of 30.57211 mins; Ouch!! That’s a long time.

print(svm_model$finalModel)


#Predict on Training set
svm_train_pred <- predict(svm_model, train)
confusionMatrix(train$isFraud, svm_train_pred, positive = "Yes")

#Accuracy : 0.9305
#Sensitivity : 0.9399
#Specificity : 0.9215
#Predict on Test set

svm_test_pred <- predict(svm_model, test)
confusionMatrix(test$isFraud, svm_test_pred, positive = "Yes")

#Accuracy : 0.9323
#Sensitivity : 0.9361
#Specificity : 0.9285
#Predict on Big No-Fraud dataset


Predict on Big No-Fraud dataset

start_time <- Sys.time()
svm_big_no_pred <- predict(svm_model, big_no_sample)
end_time <- Sys.time()
end_time - start_time
start_time <- Sys.time()
svm_big_no_pred <- predict(svm_model, big_no_sample)
end_time <- Sys.time()
end_time - start_time

#Confusion Matrix
confusionMatrix(big_no_sample$isFraud, svm_big_no_pred, positive = "Yes")

#Accuracy : 0.9341
#Sensitivity : 0.00000
#Specificity : 1.00000
#False-positives: 6588
#Run-time: 49.61945 secs

Computational intesity and poor performance rule out using SVM.

#Plot the ROC curve

svm_probs <- predict(svm_model, test, type = "prob")
svm_ROC <- roc(response = test$isFraud, 
              predictor = svm_probs$Yes, 
              levels = levels(test$isFraud))

plot(svm_ROC, col = "black")

#Area under the curve
auc(svm_ROC)

#Comparing ROC curves
plot(rpart_ROC, col = "blue")
plot(c5_ROC, col = "red", add = TRUE)
plot(rf_ROC, col = "green", add = TRUE)
plot(svm_ROC, col = "black", add = TRUE)


#From the ROC curve it looks like the choice is between C5.0 and Random Forest.
#Look at area under curves for each model

sort(c(rpart = auc(rpart_ROC), rf = auc(rf_ROC), 
  c5 = auc(c5_ROC), svm = auc(svm_ROC)))
  
  
##Random Forest metrics on Big No Fraud Dataset
#Accuracy : 0.968
#Sensitivity : 0.00000
#Specificity : 1.00000
#False-positives: 3197
#Run-time: 2.538139 secs


##C5.0 metrics on Big No Fraud Dataset
#Accuracy : 0.978
#Sensitivity : 0.00000
#Specificity : 1.00000
#False-positives: 2205
#Run-time: 5.715282 secs


###Conclusion
#As usual the decision of which model to use is not straight-forward.
#The Random Forest predicts 100,000 transaction in less than half the time that C5.0 does.
#C5.0 beats Random Forest on every other metric.
#The key metric that I would focus on is the number of false-positives since transactions for 
#Some customers would be detected even though they were not fraudulent.
#I would choose the C5.0 model to use.

######END##########